---
title: "MOM Estimation"
author: "Kartik Padmanabhan"
date: "Friday, November 6, 2015"
output:
  knitrBootstrap::bootstrap_document:
    title: "MOM Estimators"
    theme: readable
    highlight: sunburst
    theme.chooser: FALSE
    highlight.chooser: FALSE
    menu: TRUE
---

# USING METHOD OF MOMENTS TO PREDICT DATABASE PERFORMANCE METRICS

## Introduction

This article exploits statistical technique method of moments (MOM) and generalized method of moments (GMM)  to estimate database statistics from the past data. Both GMM and MOM's are parametric  unsupervised form of learning. I am restricting this to Oracle Database in this example, but it can be extended to any database system.

We initially select a distribution to model our random variable. Our random variables in database statistical parameters could be any statistic that we collect say in AWR report. To start our analysis, we collected AWR reports over several months. We then scraped these data and stored our metrics in csv formats. I have presented some csv files corresponding to these statistics in my github page.


## Parameters Of Interest

Lets say we want to use the method of moments to predict the following AWR statistics for the database Load Profile in AWR statistics. This is given in this section.


    from IPython.display import Image
    import pandas as pd
    import matplotlib.pyplot as plt
    from scipy.stats.kde import gaussian_kde
    from plotly import tools
    import scipy.stats as scs
    import plotly.plotly as py
    from plotly.graph_objs import *
    import plotly.plotly as py
    from plotly.graph_objs import *
    import numpy as np


    Image(filename='images/load_profile.png')




![png](method_of_moments_files/method_of_moments_6_0.png)



Reading our historic load profile data in the dataframe.


    df=pd.read_csv('data/TMP_AWR_LOAD_PROFILE_AGG.csv', sep='|',parse_dates=True)
    df.head()




site_id | config_id | run_id | stat_id | stat_name | stat_per_sec | stat_per_txn | stat_per_exec | stat_per_call | start_time |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 37 | 1 | 1 | 265418 | Global Cache blocks received | 4467.21 | 16.36 | NaN | NaN | 03-OCT-14 08.00.00.000000 AM |
| 1 | 37 | 1 | 1 | 265427 | Transactions | 274.49 | NaN | NaN | NaN | 03-OCT-14 08.00.00.000000 AM |
| 2 | 37 | 1 | 1 | 265410 | Logical read (blocks) | 291880.19 | 1064.87 | NaN | NaN | 03-OCT-14 08.00.00.000000 AM |
| 3 | 37 | 1 | 1 | 265419 | Global Cache blocks served | 4467.02 | 16.34 | NaN | NaN | 03-OCT-14 08.00.00.000000 AM |
| 4 | 37 | 1 | 1 | 265421 | Parses (SQL) | 1131.04 | 4.14 | NaN | NaN | 03-OCT-14 08.00.00.000000 AM |


As we can we have the data for several statistics viz. Global Cache blocks received, Transactions, Logical read (blocks), Global Cache blocks served, etc from the AWR load profile start from date 03-OCT-14  until 02-NOV-15.
I am interested in forecasting only stat_per_sec information on this AWR Load profile, so using pivot function in pandas we get the following reconstructed dataframe.      


    df=df.pivot_table(index='start_time', columns='stat_name', values='stat_per_sec')
    df.head()




<div>

| stat_name | Background CPU(s) | Block changes | DB CPU(s) | DB Time(s) | Executes (SQL) | Global Cache blocks received | Global Cache blocks served | Hard parses (SQL) | IM scan rows | Logical read (blocks) | ... | Read IO (MB) | Read IO requests | Redo size (bytes) | Rollbacks | SQL Work Area (MB) | Session Logical Read IM | Transactions | User calls | Write IO (MB) | Write IO requests |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| start_time |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 01-APR-15 08.00.00.000000 AM | 2.34 | 7171.66 | 8.34 | 21.33 | 7295.85 | 4067.55 | 4067.66 | 0.53 | 0 | 368308.41 | ... | 146.02 | 12323.87 | 2520626.66 | 204.03 | 471.86 | NaN | 342.13 | 3172.36 | 7.38 | 791.23 |
| 01-APR-15 10.00.00.000000 PM | 1.88 | 5829.02 | 5.75 | 14.98 | 4943.24 | 3371.89 | 3374.17 | 0.41 | 0 | 274450.76 | ... | 237.88 | 9459.97 | 1708287.87 | 143.23 | 282.08 | NaN | 243.01 | 2288.04 | 5.59 | 603.88 |
| 01-JAN-15 08.00.00.000000 AM | NaN | 1746.34 | 1.80 | 4.07 | 2277.13 | 1618.04 | 1617.80 | 0.30 | NaN | 119422.36 | ... | 160.56 | 3976.52 | 577855.43 | 74.32 | 113.44 | NaN | 139.59 | 970.47 | 1.61 | 162.10 |
| 01-JUL-15 08.00.00.000000 AM | 2.07 | 6530.64 | 6.92 | 18.46 | 6485.98 | 4164.01 | 4164.06 | 1.67 | 0 | 306467.96 | ... | 155.87 | 9908.58 | 2263412.36 | 198.20 | 424.66 | NaN | 334.05 | 3064.53 | 6.76 | 721.93 |
| 01-JUL-15 10.00.00.000000 PM | 1.80 | 5853.14 | 5.19 | 14.20 | 4787.97 | 3469.74 | 3469.68 | 0.94 | 0 | 248284.74 | ... | 192.59 | 8087.20 | 1731536.10 | 151.27 | 288.96 | NaN | 259.02 | 2370.28 | 5.70 | 606.48 |

5 rows × 24 columns

</div>


    print (df.columns)

    Index(['Background CPU(s)', 'Block changes', 'DB CPU(s)', 'DB Time(s)',
           'Executes (SQL)', 'Global Cache blocks received',
           'Global Cache blocks served', 'Hard parses (SQL)', 'IM scan rows',
           'Logical read (blocks)', 'Logons', 'Parses (SQL)',
           'Physical read (blocks)', 'Physical write (blocks)', 'Read IO (MB)',
           'Read IO requests', 'Redo size (bytes)', 'Rollbacks',
           'SQL Work Area (MB)', 'Session Logical Read IM', 'Transactions',
           'User calls', 'Write IO (MB)', 'Write IO requests'],
          dtype='object', name='stat_name')


## Distribution Selection

All of these database statistics are continous random variables, so we model using some continous distributions.
scipy offers several continous distributions for the purpose of modelling, viz Gamma, Normal, Anglit, Arcsine, Beta, Cauchy etc.

Once we have the estimated model, we can then use the Kolmogorov-Smirov test to evaluate your fit using scipy.stats.kstest. Lets start with the most popular gamma distribution.

Say I am interested in Background CPU(s) information, calculating the gamma distribution parameters alpha and beta.



    sample_mean=df['Background CPU(s)'].mean()
    sample_var=np.var(df['Background CPU(s)'], ddof=1)
    (sample_mean,sample_var)




    (2.1573536895674299, 0.2304741016253824)



Alphas and Betas of Gamma distribution:


    ###############
    #### Gamma ####
    ###############
    
    alpha = sample_mean**2 / sample_var
    beta = sample_mean / sample_var 
    alpha,beta




    (20.193917273426234, 9.3605037370925057)



## MOM Estimation

Using estimated parameters and plot the distribution on top of data


    gamma_rv = scs.gamma(a=alpha, scale=1/beta)


    # Get the probability for each value in the data
    x_vals = np.linspace(df['Background CPU(s)'].min(), df['Background CPU(s)'].max())
    gamma_p = gamma_rv.pdf(x_vals)


    df.head()



<div>

| stat_name | Background CPU(s) | Block changes | DB CPU(s) | DB Time(s) | Executes (SQL) | Global Cache blocks received | Global Cache blocks served | Hard parses (SQL) | IM scan rows | Logical read (blocks) | ... | Read IO (MB) | Read IO requests | Redo size (bytes) | Rollbacks | SQL Work Area (MB) | Session Logical Read IM | Transactions | User calls | Write IO (MB) | Write IO requests |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| start_time |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 01-APR-15 08.00.00.000000 AM | 2.34 | 7171.66 | 8.34 | 21.33 | 7295.85 | 4067.55 | 4067.66 | 0.53 | 0 | 368308.41 | ... | 146.02 | 12323.87 | 2520626.66 | 204.03 | 471.86 | NaN | 342.13 | 3172.36 | 7.38 | 791.23 |
| 01-APR-15 10.00.00.000000 PM | 1.88 | 5829.02 | 5.75 | 14.98 | 4943.24 | 3371.89 | 3374.17 | 0.41 | 0 | 274450.76 | ... | 237.88 | 9459.97 | 1708287.87 | 143.23 | 282.08 | NaN | 243.01 | 2288.04 | 5.59 | 603.88 |
| 01-JAN-15 08.00.00.000000 AM | NaN | 1746.34 | 1.80 | 4.07 | 2277.13 | 1618.04 | 1617.80 | 0.30 | NaN | 119422.36 | ... | 160.56 | 3976.52 | 577855.43 | 74.32 | 113.44 | NaN | 139.59 | 970.47 | 1.61 | 162.10 |
| 01-JUL-15 08.00.00.000000 AM | 2.07 | 6530.64 | 6.92 | 18.46 | 6485.98 | 4164.01 | 4164.06 | 1.67 | 0 | 306467.96 | ... | 155.87 | 9908.58 | 2263412.36 | 198.20 | 424.66 | NaN | 334.05 | 3064.53 | 6.76 | 721.93 |
| 01-JUL-15 10.00.00.000000 PM | 1.80 | 5853.14 | 5.19 | 14.20 | 4787.97 | 3469.74 | 3469.68 | 0.94 | 0 | 248284.74 | ... | 192.59 | 8087.20 | 1731536.10 | 151.27 | 288.96 | NaN | 259.02 | 2370.28 | 5.70 | 606.48 |

5 rows × 24 columns

</div>


    '''Use the estimated parameters and plot the distribution on top of data'''
    fig, ax = plt.subplots()
    # Plot those values on top of the real data
    ax = df['Background CPU(s)'].hist(bins=20, normed=1, edgecolor='none', figsize=(10, 7))
    ax.set_xlabel('Statistics')
    ax.set_ylabel('Probability Density')
    ax.set_title('Background CPU(s)')
    
    ax.plot(x_vals, gamma_p, color='r', label='Gamma', alpha=0.6)
    ax.legend()
    fig.savefig('images/backgroundcpu.png')
    #py.iplot_mpl(fig, filename='s6_log-scales')


![png](method_of_moments_files/method_of_moments_22_0.png)



    # Lets put everything in a function 
    # Define a function that plots distribution fitted to one month's of data
    def plot_mom(df, col):
        sample_mean=df[col].mean()
        sample_var=np.var(df[col], ddof=1)
        alpha = sample_mean**2 / sample_var
        beta = sample_mean / sample_var 
        gamma_rv = scs.gamma(a=alpha, scale=1/beta)
        # Get the probability for each value in the data
        x_vals = np.linspace(df[col].min(), df[col].max())
        gamma_p = gamma_rv.pdf(x_vals)
        fig, ax = plt.subplots()
        # Plot those values on top of the real data
        ax = df[col].hist(bins=20, normed=1, edgecolor='none')
        ax.set_xlabel('Statistics')
        ax.set_ylabel('Probability Density')
        ax.set_title(col)
        ax.plot(x_vals, gamma_p, color='r', label='Gamma', alpha=0.6)
        ax.legend()


    df.columns




    Index(['Background CPU(s)', 'Block changes', 'DB CPU(s)', 'DB Time(s)',
           'Executes (SQL)', 'Global Cache blocks received',
           'Global Cache blocks served', 'Hard parses (SQL)', 'IM scan rows',
           'Logical read (blocks)', 'Logons', 'Parses (SQL)',
           'Physical read (blocks)', 'Physical write (blocks)', 'Read IO (MB)',
           'Read IO requests', 'Redo size (bytes)', 'Rollbacks',
           'SQL Work Area (MB)', 'Session Logical Read IM', 'Transactions',
           'User calls', 'Write IO (MB)', 'Write IO requests'],
          dtype='object', name='stat_name')



Lets just pick the following important statistics to estimate:
	* 'Background CPU(s)'
	* 'DB CPU(s)',
	* 'Executes (SQL)',
	* 'IM scan rows',
	* 'Hard parses (SQL)',
	* 'Logical read (blocks)',
	* 'Parses (SQL)',
	* 'Physical read (blocks)',
	* 'Physical write (blocks)'


    columns_of_interest=['Background CPU(s)', 'DB CPU(s)', 'Executes (SQL)', 'User calls', 'Hard parses (SQL)', 'Logical read (blocks)', 'Parses (SQL)', 'Physical read (blocks)', 'Physical write (blocks)']


    for col in columns_of_interest:
        plot_mom(df, col)
        fig.savefig('images/'+col)


![png](method_of_moments_files/method_of_moments_27_0.png)



![png](method_of_moments_files/method_of_moments_27_1.png)



![png](method_of_moments_files/method_of_moments_27_2.png)



![png](method_of_moments_files/method_of_moments_27_3.png)



![png](method_of_moments_files/method_of_moments_27_4.png)



![png](method_of_moments_files/method_of_moments_27_5.png)



![png](method_of_moments_files/method_of_moments_27_6.png)



![png](method_of_moments_files/method_of_moments_27_7.png)



![png](method_of_moments_files/method_of_moments_27_8.png)



    df.head()




<div>

| stat_name | Background CPU(s) | Block changes | DB CPU(s) | DB Time(s) | Executes (SQL) | Global Cache blocks received | Global Cache blocks served | Hard parses (SQL) | IM scan rows | Logical read (blocks) | ... | Read IO (MB) | Read IO requests | Redo size (bytes) | Rollbacks | SQL Work Area (MB) | Session Logical Read IM | Transactions | User calls | Write IO (MB) | Write IO requests |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| start_time |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 01-APR-15 08.00.00.000000 AM | 2.34 | 7171.66 | 8.34 | 21.33 | 7295.85 | 4067.55 | 4067.66 | 0.53 | 0 | 368308.41 | ... | 146.02 | 12323.87 | 2520626.66 | 204.03 | 471.86 | NaN | 342.13 | 3172.36 | 7.38 | 791.23 |
| 01-APR-15 10.00.00.000000 PM | 1.88 | 5829.02 | 5.75 | 14.98 | 4943.24 | 3371.89 | 3374.17 | 0.41 | 0 | 274450.76 | ... | 237.88 | 9459.97 | 1708287.87 | 143.23 | 282.08 | NaN | 243.01 | 2288.04 | 5.59 | 603.88 |
| 01-JAN-15 08.00.00.000000 AM | NaN | 1746.34 | 1.80 | 4.07 | 2277.13 | 1618.04 | 1617.80 | 0.30 | NaN | 119422.36 | ... | 160.56 | 3976.52 | 577855.43 | 74.32 | 113.44 | NaN | 139.59 | 970.47 | 1.61 | 162.10 |
| 01-JUL-15 08.00.00.000000 AM | 2.07 | 6530.64 | 6.92 | 18.46 | 6485.98 | 4164.01 | 4164.06 | 1.67 | 0 | 306467.96 | ... | 155.87 | 9908.58 | 2263412.36 | 198.20 | 424.66 | NaN | 334.05 | 3064.53 | 6.76 | 721.93 |
| 01-JUL-15 10.00.00.000000 PM | 1.80 | 5853.14 | 5.19 | 14.20 | 4787.97 | 3469.74 | 3469.68 | 0.94 | 0 | 248284.74 | ... | 192.59 | 8087.20 | 1731536.10 | 151.27 | 288.96 | NaN | 259.02 | 2370.28 | 5.70 | 606.48 |

5 rows × 24 columns

</div>


This above function would generate the MOM estimate for any database statistic that we would want.
For example if we want to predict MOM's for "Global Cache blocks served", we just need to call this function:


    plot_mom(df,'Global Cache blocks received')
    fig.savefig('images/cache_blocks.png')


![png](method_of_moments_files/method_of_moments_30_0.png)


For "Rollbacks" example our estimation is:


    plot_mom(df,'Rollbacks')
    fig.savefig('images/rollbacks.png')


![png](method_of_moments_files/method_of_moments_32_0.png)


## Conclusion

As we can we our estimations (plotted in red) are pretty good estimation of our actual values that we observe in our real outputs. This concludes our discussion on MOM estimators on database statistics.
